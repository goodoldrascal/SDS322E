---
title: "Project 2 Report"
output: html_document
---

## Group Members

Specify the names and EIDs of all group members:Nicholas Rhyan Ncr828, David Paul Salinas Dps2387, Kieran Givens ktg557




## Question

State the question being addressed and describe the outcome variable to be predicted. Explain why it is useful to answer this question.

**We sought to determine whether injuries resulting from a motor vehicle crash could be predicted using variables not directly encoding injury severity or injury counts. To address this, we reframed the outcome as a binary classification problem by constructing a new indicator variable equal to TRUE if a crash involved at least one injury and FALSE if no injuries were reported. Importantly, the outcome variable was reasonably balanced: approximately 45% of crashes in the dataset involved at least one injury.**


## Data and Setup

Load the dataset into R below. In addition, load any packages that you will need using the `library()` function. For example, you will need to load the `tidyverse` and `tidymodels` packages.


```{r}
## Add your code for reading in the data here

## If the dataset is too large for your computer to handle, you can draw a
## smaller random sub-sample of the data using the `sample_n()` function. For
## example 
##
## dat_sub <- sample_n(dat, 50000) 
## 
## draws a random sample of 50,000 rows from the `dat` object.
library(future)
plan(multisession, workers = 8)
library(tidyverse)
library(readr)
library(tidymodels)
df <- read_csv("Austin_Crash_Report_Data_-_Crash_Level_Records_20250407.csv.gz")
# df <- Austin_Crash_Report_Data_._Crash_Level_Records_20250407.csv
df <- df |> 
  mutate(injury = factor(tot_injry_cnt > 0, levels = c(FALSE, TRUE), labels = c("no_injury", "injury"))) |> 
  relocate(injury, .before = 1)
df |>
  select(injury) |>
  ggplot(aes(x = injury)) +
  geom_bar() +
  labs(
    title = "Injury Indicator",
    x = "Injury (TRUE/FALSE)",
    y = "Count"
  ) +
  theme_minimal()
colnames(df)
```
```{r}
head(df)
```


## Base Prediction Model

Use either linear regression (for a continuous outcome) or logistic regression (for a binary outcome) to build a base prediction model. Use the tidymodels framework to 

1. Split the dataset into a training and testing dataset using 75% of observations for training and 25% for testing; leave the test dataset aside for now.

2. Select variables to include in your model recipe. You may need to experiment with different variable combinations to find the best predicting model.

Use 10-fold cross-validation on the training data to estimate either 

* the root mean squared error (RMSE) if your outcome is continuous; or 
* the accuracy if your outcome is binary.


```{r}
## Add your code here
set.seed(119)
df <- df |> 
  mutate(
    onsys_fl = factor(onsys_fl),
    private_dr_fl = factor(private_dr_fl)
  )

df <- df |>
  mutate(
    crash_time = lubridate::mdy_hms(`Crash timestamp (US/Central)`),
    hour  = lubridate::hour(crash_time),
    wday  = lubridate::wday(crash_time, label = TRUE),
    month = lubridate::month(crash_time, label = TRUE)
  )

head(df)
```


```{r}



df_split <- initial_split(df, prop = 0.75)
train <- training(df_split)
test  <- testing(df_split)

## Recipe (cleaned: NO crash_time)
rec <- recipe(
  injury ~ latitude + longitude + crash_speed_limit +
           onsys_fl + road_constr_zone_fl +
           hour + wday + month,
  data = train
)
model <- logistic_reg() |>
  set_engine("glm")

wf <- workflow() |>
  add_recipe(rec) |>
  add_model(model)

folds <- vfold_cv(train, v = 10)

res <- fit_resamples(wf, resamples = folds)



log_metrics <- res |> collect_metrics()

write_csv(log_metrics, "~/projects/Project2/log_metrics.csv")

log_metrics

```

Which configuration of variables in your model seems to produce the best performing model on the training data?

**We found that through trying out a mixture of the variables that made the most sense to us, there was miniscule change in the performance of the model overall. The variables we ended up using most recently were latitude, longitude, crash speed limit, onsys_fl, road_constr_zone_fl, hour, wday, month**



## Alternate Prediction Model

For this part of the analysis, choose one machine learning approach to build an alternate prediction model. The ones we have seen in class are K-Nearest Neighbors (`nearest_neighbor()`), decision trees (`decision_tree()`), and random forests (`rand_forest()`). 

1. Identify the tuning parameters for your chosen model.

2. Explore which combination of variables and tuning parameters produces good model performance.

3. Create a workflow that assembles your recipe and your model specification.

4. Use 10-fold cross-validation to estimate either the RMSE (continuous outcome) or the accuracy (binary outcome) for your model in the training data.

5. Tune your model using a range of possible tuning parameters to identify the best-predicting model.

NOTE: Doing the cross-validation and model tuning here may take a *significant* amount of time depending on your choice of tuning parameters and the speed of your computer. You may need to be patient with the process here.

```{r}
library(vip)

## Perform feature importance to determine what variables to include for final model
rf_fit <- rand_forest(mode = "classification") |>
  set_engine("ranger", importance = "impurity") |>
  fit(injury ~ latitude + longitude + crash_speed_limit +
           onsys_fl + units_involved + road_constr_zone_fl +
           hour + wday + month, data = train)

vip(rf_fit)
```


```{r}
library(tidymodels)
library(ranger)

## Recipe (cleaned: NO crash_time)
rec <- recipe(
  injury ~ latitude + longitude + crash_speed_limit +
           + units_involved + hour + wday + month, data = train
)

## Random forest model
model <- rand_forest(
  mtry = 6,
  min_n = 20,
  trees = 1000
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

## Workflow
wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(model)

# Fit the finalized model to the full training data
final_fit <- fit(wf, train)

# --- 7. EVALUATE ON TEST SET ---
preds <- predict(final_fit, test, type = "prob") %>%
  bind_cols(test)

# Calculate final ROC AUC
roc_auc(preds, truth = injury, .pred_injury)
```

Which one of your models (i.e., which combination of tuning parameters and variables) produces the best prediction performance on the training dataset?

**An m_try of 6 and min_n of 20 were found to be the best tuning parameters. The best variables were found to be latitude, longitude, crash speed limit, units involved, hour, weekday, and month**


## Final Model Fit

Choose the model that performs the best on your training dataset (either the linear/logistic regression or the machine learning model). Make a final assessment of your model using the testing dataset.


```{r}
linear_model <- logistic_reg() %>%
  set_engine("glm")

# 2. Reset the Workflow 
wf_linear <- workflow() %>% 
  add_recipe(rec) %>% 
  add_model(linear_model)

glm_fit <- fit(wf_linear, data = train) 

# 4. Predict and Evaluate
preds <- predict(glm_fit, test, type = "prob") %>%
  bind_cols(predict(glm_fit, test, type = "class")) %>%
  bind_cols(test)

# Metrics
roc_auc(preds, truth = injury, .pred_injury)
accuracy(preds, truth = injury, .pred_class)
```


What are the final performance metrics for your best model?

**The final performance metrics are .402 for AUC and .595 for accuracy**



## Model Improvement

Suppose you had the ability to improve the fit of the model by collecting data on a new variable. What data would you most like to collect to improve the prediction performance of the model? Explain why you think collecting this new variable would improve the prediction performance. You may optionally include some code/analysis to support your explanation.

```{r}
## Optional: Add any supporting code here

```

**We would most like to collect data on what specific model of the cars involved are. We think that this would be beneficial to predict whether there is an injury or not as there are car models that aren't as "safe" as other models. Someone driving something like a Kia Soul would be much more likely to sustain an injury in comparison to someone driving a bigger vehicle like a Ford F150.**




## Discussion

Reflect on the process of conducting this project.

What was challenging?

What have you learned from the process itself?

Was there anything that was unexpected that you found during this analysis? If so, what was your expectation and how did the experience deviate from your expectation?

**The biggest challenge during this project was getting the full workflow to run smoothly with a large data set. Cross-validation and parameter tuning was more computationally intense than expected. As well, feature engineering decisions were often not obvious. Many variables could be represented in multiple ways, and it wasn't clear ahead of time which choices would improve prediction performance versus adding noise.**

**We learned how to build models in a structured and reproducible way following a clear workflow. We also learned how integral to model performance that preprocessing choices are, handling missing values, converting variables correctly, and engineering features often mattered as much as the model choice.**

**One unexpected results was that our baseline logistic regression only produced small improvements in performance when using a similar set of predictors. We expected random forest to outperform logistic regression by a wide margin, but the gain was somewhat modest, suggesting that the features may not contain the strongest signal for predicting injury.**

**





## Submission to Gradescope

**Make sure to add your group members to the Gradescope submission!**
