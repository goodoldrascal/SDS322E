---
title: "Project 2 Report"
output: html_document
---

## Group Members

Specify the names and EIDs of all group members:



## Question

State the question being addressed and describe the outcome variable to be predicted. Explain why it is useful to answer this question.

**Write your answer here.**


## Data and Setup

Load the dataset into R below. In addition, load any packages that you will need using the `library()` function. For example, you will need to load the `tidyverse` and `tidymodels` packages.


```{r}
## Add your code for reading in the data here

## If the dataset is too large for your computer to handle, you can draw a
## smaller random sub-sample of the data using the `sample_n()` function. For
## example 
##
## dat_sub <- sample_n(dat, 50000) 
## 
## draws a random sample of 50,000 rows from the `dat` object.
library(future)
plan(multisession, workers = 8)
library(tidyverse)
library(readr)
library(tidymodels)
df <- read_csv("Austin_Crash_Report_Data_-_Crash_Level_Records_20250407.csv.gz")
# df <- Austin_Crash_Report_Data_._Crash_Level_Records_20250407.csv
df <- df |> 
  mutate(injury = factor(tot_injry_cnt > 0, levels = c(FALSE, TRUE), labels = c("no_injury", "injury"))) |> 
  relocate(injury, .before = 1)
df |>
  select(injury) |>
  ggplot(aes(x = injury)) +
  geom_bar() +
  labs(
    title = "Injury Indicator",
    x = "Injury (TRUE/FALSE)",
    y = "Count"
  ) +
  theme_minimal()
colnames(df)
```
```{r}
head(df)
```


## Base Prediction Model

Use either linear regression (for a continuous outcome) or logistic regression (for a binary outcome) to build a base prediction model. Use the tidymodels framework to 

1. Split the dataset into a training and testing dataset using 75% of observations for training and 25% for testing; leave the test dataset aside for now.

2. Select variables to include in your model recipe. You may need to experiment with different variable combinations to find the best predicting model.

Use 10-fold cross-validation on the training data to estimate either 

* the root mean squared error (RMSE) if your outcome is continuous; or 
* the accuracy if your outcome is binary.


```{r}
## Add your code here
set.seed(119)
df <- df |> 
  mutate(
    onsys_fl = factor(onsys_fl),
    private_dr_fl = factor(private_dr_fl)
  )

df <- df |>
  mutate(
    crash_time = lubridate::mdy_hms(`Crash timestamp (US/Central)`),
    hour  = lubridate::hour(crash_time),
    wday  = lubridate::wday(crash_time, label = TRUE),
    month = lubridate::month(crash_time, label = TRUE)
  )

head(df)
```


```{r}



df_split <- initial_split(df, prop = 0.75)
train <- training(df_split)
test  <- testing(df_split)

## Recipe (cleaned: NO crash_time)
rec <- recipe(
  injury ~ latitude + longitude + crash_speed_limit +
           onsys_fl + road_constr_zone_fl +
           hour + wday + month,
  data = train
)
model <- logistic_reg() |>
  set_engine("glm")

wf <- workflow() |>
  add_recipe(rec) |>
  add_model(model)

folds <- vfold_cv(train, v = 10)

res <- fit_resamples(wf, resamples = folds)



log_metrics <- res |> collect_metrics()

write_csv(log_metrics, "~/projects/Project2/log_metrics.csv")

log_metrics

```

Which configuration of variables in your model seems to produce the best performing model on the training data?

**We found that through trying out a mixture of the variables that made the most sense to us, there was miniscule change in the performance of the model overall. The variables we ended up using most recently were latitude, longitude, crash speed limit, onsys_fl, road_constr_zone_fl, hour, wday, month**



## Alternate Prediction Model

For this part of the analysis, choose one machine learning approach to build an alternate prediction model. The ones we have seen in class are K-Nearest Neighbors (`nearest_neighbor()`), decision trees (`decision_tree()`), and random forests (`rand_forest()`). 

1. Identify the tuning parameters for your chosen model.

2. Explore which combination of variables and tuning parameters produces good model performance.

3. Create a workflow that assembles your recipe and your model specification.

4. Use 10-fold cross-validation to estimate either the RMSE (continuous outcome) or the accuracy (binary outcome) for your model in the training data.

5. Tune your model using a range of possible tuning parameters to identify the best-predicting model.

NOTE: Doing the cross-validation and model tuning here may take a *significant* amount of time depending on your choice of tuning parameters and the speed of your computer. You may need to be patient with the process here.

```{r}
library(vip)

## Perform feature importance to determine what variables to include for final model
rf_fit <- rand_forest(mode = "classification") |>
  set_engine("ranger", importance = "impurity") |>
  fit(injury ~ latitude + longitude + crash_speed_limit +
           onsys_fl + units_involved + road_constr_zone_fl +
           hour + wday + month, data = train)

vip(rf_fit)
```


```{r}
library(tidymodels)
library(ranger)


## Random forest model
model <- rand_forest(
  mtry = tune(),
  min_n = tune(),
  trees = 1000
) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

## Workflow
wf <- workflow() %>%
  add_recipe(rec) %>%
  add_model(model)

## CV folds (not used now but not harmful)
set.seed(123)
folds <- vfold_cv(train, v = 10)


## Select the best RF hyperparameter configuration
best_res <- select_best(res2, metric = "roc_auc")
best_res
=======
rf_grid <- grid_regular(
  mtry(range = c(2, 7)),   # Try splitting on 2 columns up to 7 columns
  min_n(range = c(5, 40)), # Try deep trees (5) vs shallow trees (40)
  levels = 5               # Try 5 values for each (Total = 25 combinations)
)

tune_res <- tune_grid(
  wf,
  resamples = folds,
  grid = rf_grid
)

tune_res |> collect_metrics()

preds <- predict(final_fit, test, type = "prob") %>%
  bind_cols(test)

roc_auc(preds, truth = injury, .pred_injury)

rf_metrics <- collect_metrics(res2)
write_csv(rf_metrics, "~/projects/Project2/rf_metrics.csv")

rf_metrics
=======
# ## Load previously saved tuning results
# res2 <- readRDS("/stor/scratch/WCAAR/rhyan_scratch/rf_tuning_output/tuning_results.rds")
# 
# ## Select the best RF hyperparameter configuration
# best_res <- select_best(res2, metric = "roc_auc")
# best_res
# 
# ## Finalize workflow with optimal tuning params
# final_wf <- finalize_workflow(wf, best_res)
# 
# ## Fit the finalized RF to the *full training data*
# final_fit <- fit(final_wf, train)
# 
# preds <- predict(final_fit, test, type = "prob") %>%
#   bind_cols(test)
# 
# roc_auc(preds, truth = injury, .pred_injury)
# 
# collect_metrics(res2)
```

Which one of your models (i.e., which combination of tuning parameters and variables) produces the best prediction performance on the training dataset?

**Write your answer here.**


## Final Model Fit

Choose the model that performs the best on your training dataset (either the linear/logistic regression or the machine learning model). Make a final assessment of your model using the testing dataset.


```{r}
## Add your code here


```


What are the final performance metrics for your best model?

**Write your answer here.**



## Model Improvement

Suppose you had the ability to improve the fit of the model by collecting data on a new variable. What data would you most like to collect to improve the prediction performance of the model? Explain why you think collecting this new variable would improve the prediction performance. You may optionally include some code/analysis to support your explanation.

```{r}
## Optional: Add any supporting code here

```

**Write your answer here.**




## Discussion

Reflect on the process of conducting this project.

What was challenging?

What have you learned from the process itself?

Was there anything that was unexpected that you found during this analysis? If so, what was your expectation and how did the experience deviate from your expectation?

**Write your answer here.**





## Submission to Gradescope

**Make sure to add your group members to the Gradescope submission!**
